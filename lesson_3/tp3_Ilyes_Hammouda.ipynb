{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST with an MLP, from scratch\n",
    "\n",
    "# - Step 1: build an MLP from scratch to solve MNIST. Question set: https://fleuret.org/dlc/materials/dlc-practical-3.pdf\n",
    "# - Step 2: debug your network with backprop ninja and a reference implementation using torch's .backward()\n",
    "# - Step 3: build the same MLP but will full pytorch code (nn.Linear, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = load_data(one_hot_labels = True, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdb400b3c10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[4].view((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy (preds, targets):\n",
    "    \"\"\" Computes the accuracy between predictions and targets. Data is expected to be one-hot encoded. \"\"\"\n",
    "    _, idx1 = torch.max(preds, dim=1)\n",
    "    _, idx2 = torch.max(targets, dim=1)\n",
    "    d = idx1 == idx2\n",
    "    return d.int().float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test\n",
    "# this cell should return 0.75\n",
    "preds = torch.zeros((4,7))\n",
    "preds[0,1] = 1\n",
    "preds[1,4] = 1\n",
    "preds[2,2] = 1\n",
    "preds[3,6] = 1\n",
    "targets = torch.zeros((4,7))\n",
    "targets[0,1] = 1\n",
    "targets[1,4] = 1\n",
    "targets[2,2] = 1\n",
    "targets[3,2] = 1\n",
    "compute_accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def dsigma(x):\n",
    "    return 1 - torch.pow(sigma(x), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss (v,t):\n",
    "    return(torch.sum (torch.pow(v-t,2)))\n",
    "\n",
    "def dloss(v,t):\n",
    "    return torch.mul(2, (v-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5529,  1.1427, -0.1610, -2.0885, -3.8716, -4.2020],\n",
       "        [ 1.1891,  2.0751,  4.8847,  1.2422, -1.4316, -4.5210],\n",
       "        [ 5.5198, -1.9320, -1.5009, -2.7508,  0.8101,  1.0077]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "v = torch.randn((3, 6), dtype=torch.float32)\n",
    "t = torch.randn((3, 6), dtype=torch.float32)\n",
    "l=loss(v,t)\n",
    "dloss(v,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply targets by 0.9 to be in the range of tanh\n",
    "train_target *= 0.9\n",
    "test_target *= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "# DO NOT MODIFY IT\n",
    "#\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "w1 = torch.ones((784, 50), requires_grad=True)\n",
    "b1 = torch.ones((50), requires_grad=True)\n",
    "w2 = torch.ones((50, 10), requires_grad=True)\n",
    "b2 = torch.ones((10), requires_grad=True)\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]), tensor(55.8500, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = train_input[:5]\n",
    "y1 = train_target[:5]\n",
    "z1 = x1 @ w1 + b1.t()\n",
    "h1 = sigma(z1)\n",
    "z2 = h1 @ w2 + b2.t()\n",
    "h2 = sigma(z2)\n",
    "l = loss(h2, y1)\n",
    "h2.shape, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=55.849998474121094\n"
     ]
    }
   ],
   "source": [
    "# Force pytorch to retain grade for intermediate nodes and reset grad for parameters\n",
    "# DO NOT MODIFY THIS CODE\n",
    "#\n",
    "others = [h2,z2,h1,z1]\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in others:\n",
    "    t.retain_grad()\n",
    "l.backward()\n",
    "print(f'loss={l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "z2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "w2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "z1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "w1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# here we compare our gradient to the reference gradient computed by pytorch\n",
    "dl = 1.0\n",
    "dh2 = dloss(h2, y1)\n",
    "cmp('h2',dh2,h2)\n",
    "dz2 = torch.mul(dh2, dsigma(z2))\n",
    "cmp('z2',dz2, z2)\n",
    "dw2 = h1.t() @ dz2\n",
    "cmp('w2',dw2, w2)\n",
    "db2 = dz2.sum(0).reshape(b2.shape)\n",
    "cmp('b2',db2, b2)\n",
    "dh1 = dz2 @ w2.t()\n",
    "cmp('h1',dh1, h1)\n",
    "dz1 = torch.mul(dh1, dsigma(z1))\n",
    "cmp('z1', dz1, z1)\n",
    "dw1 = x1.t() @ dz1\n",
    "cmp('w1', dw1, w1)\n",
    "db1 = dz1.sum(0).reshape(b1.shape)\n",
    "cmp('b1', db1, b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "with torch.no_grad():\n",
    "    w1 += -lr * dw1\n",
    "    #print(f'b1:{b1.shape}, \\n db1:{db1.squeeze().shape}')\n",
    "    b1 += -lr * db1.squeeze()\n",
    "    w2 += -lr * dw2\n",
    "    b2 += -lr * db2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.849998474121094"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = loss(h2, y1)\n",
    "l.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we've checked our gradients are correct, we can implement the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w1, b1, w2, b2, x):\n",
    "    z1 = x @ w1 + b1.t()\n",
    "    h1 = sigma(z1)\n",
    "    z2 = h1 @ w2 + b2.t()\n",
    "    h2 = sigma(z2)\n",
    "    return z1, h1, z2, h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(w1, b1, w2, b2, x1, y1, h2, z2, h1, z1):\n",
    "    dl = 1.0\n",
    "    dh2 = dloss(h2, y1)\n",
    "    dz2 = torch.mul(dh2, dsigma(z2))\n",
    "    dw2 = h1.t() @ dz2\n",
    "    db2 = dz2.sum(0).reshape(b2.shape)\n",
    "    dh1 = dz2 @ w2.t()\n",
    "    dz1 = torch.mul(dh1, dsigma(z1))\n",
    "    dw1 = x1.t() @ dz1\n",
    "    db1 = dz1.sum(0).reshape(b1.shape)\n",
    "        \n",
    "    return dw1, db1, dw2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w1, b1, w2, b2, dw1, db1, dw2, db2, lr):\n",
    "    with torch.no_grad():\n",
    "        w1 += -lr * dw1\n",
    "        b1 += -lr * db1.squeeze()\n",
    "        w2 += -lr * dw2\n",
    "        b2 += -lr * db2.squeeze()\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \"\"\" init a network \"\"\"\n",
    "    torch.manual_seed(1337)\n",
    "    w1 = torch.randn((784, 50), requires_grad=True)\n",
    "    b1 = torch.randn((50), requires_grad=True)\n",
    "    w2 = torch.randn((50, 10), requires_grad=True)\n",
    "    b2 = torch.randn((10), requires_grad=True)\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = init()\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "torch.set_printoptions(linewidth=200)\n",
    "def train(w1, b1, w2, b2):\n",
    "    lossi = []\n",
    "    for step in range(10000):\n",
    "        xb = train_input\n",
    "        yb = train_target\n",
    "        num_samples = xb.shape[0]\n",
    "        #print(f'{num_samples=}')\n",
    "        # forward\n",
    "        z1, h1, z2, h2 = forward(w1, b1, w2, b2, xb)\n",
    "        lsi = loss(h2, yb)\n",
    "        # backward\n",
    "        dw1, db1, dw2, db2 = backward(w1, b1, w2, b2, xb, yb, h2, z2, h1, z1)\n",
    "        # update\n",
    "        lr = 1 / num_samples if step < 5000 else 0.5 / num_samples\n",
    "        w1, b1, w2, b2 = update(w1, b1, w2, b2, dw1, db1, dw2, db2, lr)\n",
    "        if step % 100 == 0: print(f'step = {step}, loss = {lsi}')\n",
    "        lossi.append(lsi.item())\n",
    "    # compute accuracy\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, train_input)\n",
    "    train_accuracy = compute_accuracy(preds, train_target)\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, test_input)\n",
    "    test_accuracy = compute_accuracy(preds, test_target)\n",
    "    print(f'{train_accuracy=}')\n",
    "    print(f'{test_accuracy=}')\n",
    "    return lossi\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 9739.25390625\n",
      "step = 100, loss = 6926.8564453125\n",
      "step = 200, loss = 5066.0048828125\n",
      "step = 300, loss = 2349.601318359375\n",
      "step = 400, loss = 1936.7327880859375\n",
      "step = 500, loss = 1799.7366943359375\n",
      "step = 600, loss = 1672.9453125\n",
      "step = 700, loss = 1545.924560546875\n",
      "step = 800, loss = 1472.913330078125\n",
      "step = 900, loss = 1410.3072509765625\n",
      "step = 1000, loss = 1332.910888671875\n",
      "step = 1100, loss = 1325.2740478515625\n",
      "step = 1200, loss = 1234.8121337890625\n",
      "step = 1300, loss = 1198.6011962890625\n",
      "step = 1400, loss = 1163.83837890625\n",
      "step = 1500, loss = 1118.9132080078125\n",
      "step = 1600, loss = 1095.936767578125\n",
      "step = 1700, loss = 1077.3585205078125\n",
      "step = 1800, loss = 1056.1826171875\n",
      "step = 1900, loss = 1043.826416015625\n",
      "step = 2000, loss = 1032.81787109375\n",
      "step = 2100, loss = 1044.0968017578125\n",
      "step = 2200, loss = 1005.797607421875\n",
      "step = 2300, loss = 1007.5702514648438\n",
      "step = 2400, loss = 983.1707763671875\n",
      "step = 2500, loss = 990.5526123046875\n",
      "step = 2600, loss = 985.58154296875\n",
      "step = 2700, loss = 986.205810546875\n",
      "step = 2800, loss = 980.1298217773438\n",
      "step = 2900, loss = 942.627197265625\n",
      "step = 3000, loss = 973.6118774414062\n",
      "step = 3100, loss = 970.022216796875\n",
      "step = 3200, loss = 931.97509765625\n",
      "step = 3300, loss = 927.9027709960938\n",
      "step = 3400, loss = 919.6354370117188\n",
      "step = 3500, loss = 937.8753051757812\n",
      "step = 3600, loss = 916.7745361328125\n",
      "step = 3700, loss = 916.4827270507812\n",
      "step = 3800, loss = 920.5328369140625\n",
      "step = 3900, loss = 908.5330200195312\n",
      "step = 4000, loss = 890.68212890625\n",
      "step = 4100, loss = 891.7412109375\n",
      "step = 4200, loss = 880.3731689453125\n",
      "step = 4300, loss = 897.5087280273438\n",
      "step = 4400, loss = 868.2491455078125\n",
      "step = 4500, loss = 854.9378662109375\n",
      "step = 4600, loss = 861.3590698242188\n",
      "step = 4700, loss = 860.2177124023438\n",
      "step = 4800, loss = 872.4214477539062\n",
      "step = 4900, loss = 877.8362426757812\n",
      "step = 5000, loss = 854.16015625\n",
      "step = 5100, loss = 278.2513122558594\n",
      "step = 5200, loss = 274.7060241699219\n",
      "step = 5300, loss = 272.510498046875\n",
      "step = 5400, loss = 270.85174560546875\n",
      "step = 5500, loss = 269.56390380859375\n",
      "step = 5600, loss = 268.3197021484375\n",
      "step = 5700, loss = 267.3102111816406\n",
      "step = 5800, loss = 266.31988525390625\n",
      "step = 5900, loss = 265.3005065917969\n",
      "step = 6000, loss = 264.39154052734375\n",
      "step = 6100, loss = 263.433837890625\n",
      "step = 6200, loss = 262.5508117675781\n",
      "step = 6300, loss = 261.7678527832031\n",
      "step = 6400, loss = 260.9667053222656\n",
      "step = 6500, loss = 260.25433349609375\n",
      "step = 6600, loss = 259.5250244140625\n",
      "step = 6700, loss = 258.8526306152344\n",
      "step = 6800, loss = 258.24774169921875\n",
      "step = 6900, loss = 257.7120361328125\n",
      "step = 7000, loss = 257.1999816894531\n",
      "step = 7100, loss = 256.6391906738281\n",
      "step = 7200, loss = 256.1173400878906\n",
      "step = 7300, loss = 255.63748168945312\n",
      "step = 7400, loss = 255.15228271484375\n",
      "step = 7500, loss = 254.64918518066406\n",
      "step = 7600, loss = 254.1457061767578\n",
      "step = 7700, loss = 253.67605590820312\n",
      "step = 7800, loss = 253.23373413085938\n",
      "step = 7900, loss = 252.7824249267578\n",
      "step = 8000, loss = 252.30206298828125\n",
      "step = 8100, loss = 251.80816650390625\n",
      "step = 8200, loss = 251.39572143554688\n",
      "step = 8300, loss = 251.0115966796875\n",
      "step = 8400, loss = 250.64959716796875\n",
      "step = 8500, loss = 250.308837890625\n",
      "step = 8600, loss = 249.96902465820312\n",
      "step = 8700, loss = 249.6345672607422\n",
      "step = 8800, loss = 249.30467224121094\n",
      "step = 8900, loss = 248.927490234375\n",
      "step = 9000, loss = 248.5615692138672\n",
      "step = 9100, loss = 248.17459106445312\n",
      "step = 9200, loss = 247.65164184570312\n",
      "step = 9300, loss = 247.12103271484375\n",
      "step = 9400, loss = 246.6732177734375\n",
      "step = 9500, loss = 246.26004028320312\n",
      "step = 9600, loss = 245.95175170898438\n",
      "step = 9700, loss = 245.69561767578125\n",
      "step = 9800, loss = 245.4501495361328\n",
      "step = 9900, loss = 245.22329711914062\n",
      "train_accuracy=0.9580000042915344\n",
      "test_accuracy=0.6330000162124634\n"
     ]
    }
   ],
   "source": [
    "lossi = train(w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb7099c130>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "#print(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reference implementation using pytorch's .backward()\n",
    "Nothing to do in Step 2, this code is provided for you as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = init()\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference code\n",
    "torch.set_printoptions(linewidth=200)\n",
    "import torch.nn as F\n",
    "\n",
    "def train(w1, b1, w2, b2):\n",
    "    lossi = []\n",
    "    for step in range(10000):\n",
    "        xb = train_input\n",
    "        yb = train_target\n",
    "        num_samples = xb.shape[0]\n",
    "        # forward\n",
    "        z1, h1, z2, h2 = forward(w1, b1, w2, b2, xb)\n",
    "        xloss = F.MSELoss()\n",
    "        lsi = xloss(h2, yb) * yb.nelement()\n",
    "        # backward\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        lsi.backward()\n",
    "        # update\n",
    "        lr = 0.1 / num_samples\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "        if step % 100 == 0: print(f'step = {step}, loss = {lsi}')\n",
    "        lossi.append(lsi.item())\n",
    "    # compute accuracy\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, train_input)\n",
    "    train_accuracy = compute_accuracy(preds, train_target)\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, test_input)\n",
    "    test_accuracy = compute_accuracy(preds, test_target)\n",
    "    print(f'{train_accuracy=}')\n",
    "    print(f'{test_accuracy=}')\n",
    "    return lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 9739.25390625\n",
      "step = 100, loss = 8384.5673828125\n",
      "step = 200, loss = 7972.76953125\n",
      "step = 300, loss = 7747.9619140625\n",
      "step = 400, loss = 7546.99951171875\n",
      "step = 500, loss = 7377.88916015625\n",
      "step = 600, loss = 7153.6015625\n",
      "step = 700, loss = 7043.0703125\n",
      "step = 800, loss = 6886.37255859375\n",
      "step = 900, loss = 6764.03564453125\n",
      "step = 1000, loss = 6646.9951171875\n",
      "step = 1100, loss = 6522.130859375\n",
      "step = 1200, loss = 6405.4970703125\n",
      "step = 1300, loss = 6234.9443359375\n",
      "step = 1400, loss = 5988.87646484375\n",
      "step = 1500, loss = 5528.62841796875\n",
      "step = 1600, loss = 5303.3173828125\n",
      "step = 1700, loss = 4997.66455078125\n",
      "step = 1800, loss = 4494.41650390625\n",
      "step = 1900, loss = 3947.8857421875\n",
      "step = 2000, loss = 3221.61865234375\n",
      "step = 2100, loss = 2476.833984375\n",
      "step = 2200, loss = 2161.94091796875\n",
      "step = 2300, loss = 2116.81640625\n",
      "step = 2400, loss = 2061.27099609375\n",
      "step = 2500, loss = 1989.8192138671875\n",
      "step = 2600, loss = 1889.899169921875\n",
      "step = 2700, loss = 1696.317626953125\n",
      "step = 2800, loss = 1427.393310546875\n",
      "step = 2900, loss = 1375.9051513671875\n",
      "step = 3000, loss = 1275.052978515625\n",
      "step = 3100, loss = 1030.0321044921875\n",
      "step = 3200, loss = 905.3505859375\n",
      "step = 3300, loss = 880.0733642578125\n",
      "step = 3400, loss = 857.3619384765625\n",
      "step = 3500, loss = 839.9254150390625\n",
      "step = 3600, loss = 825.3715209960938\n",
      "step = 3700, loss = 812.1483154296875\n",
      "step = 3800, loss = 798.5679321289062\n",
      "step = 3900, loss = 786.5311889648438\n",
      "step = 4000, loss = 774.4191284179688\n",
      "step = 4100, loss = 761.3651733398438\n",
      "step = 4200, loss = 746.956787109375\n",
      "step = 4300, loss = 732.3687744140625\n",
      "step = 4400, loss = 716.1173706054688\n",
      "step = 4500, loss = 697.29052734375\n",
      "step = 4600, loss = 671.4271240234375\n",
      "step = 4700, loss = 620.6925048828125\n",
      "step = 4800, loss = 472.6708068847656\n",
      "step = 4900, loss = 416.272705078125\n",
      "step = 5000, loss = 411.2613220214844\n",
      "step = 5100, loss = 407.9990234375\n",
      "step = 5200, loss = 405.3293762207031\n",
      "step = 5300, loss = 402.9078063964844\n",
      "step = 5400, loss = 400.74139404296875\n",
      "step = 5500, loss = 398.70770263671875\n",
      "step = 5600, loss = 396.58001708984375\n",
      "step = 5700, loss = 394.5198059082031\n",
      "step = 5800, loss = 392.47686767578125\n",
      "step = 5900, loss = 390.37420654296875\n",
      "step = 6000, loss = 388.45050048828125\n",
      "step = 6100, loss = 386.7522888183594\n",
      "step = 6200, loss = 385.1085205078125\n",
      "step = 6300, loss = 383.506591796875\n",
      "step = 6400, loss = 381.912841796875\n",
      "step = 6500, loss = 380.3619689941406\n",
      "step = 6600, loss = 378.7466735839844\n",
      "step = 6700, loss = 377.0234069824219\n",
      "step = 6800, loss = 375.48944091796875\n",
      "step = 6900, loss = 374.158447265625\n",
      "step = 7000, loss = 372.8988037109375\n",
      "step = 7100, loss = 371.6591796875\n",
      "step = 7200, loss = 370.4399108886719\n",
      "step = 7300, loss = 369.3041687011719\n",
      "step = 7400, loss = 368.2497863769531\n",
      "step = 7500, loss = 367.21484375\n",
      "step = 7600, loss = 366.1543273925781\n",
      "step = 7700, loss = 365.0657958984375\n",
      "step = 7800, loss = 363.9915466308594\n",
      "step = 7900, loss = 362.9375\n",
      "step = 8000, loss = 361.9556884765625\n",
      "step = 8100, loss = 361.03143310546875\n",
      "step = 8200, loss = 360.1446228027344\n",
      "step = 8300, loss = 359.2684631347656\n",
      "step = 8400, loss = 358.3663024902344\n",
      "step = 8500, loss = 357.3663024902344\n",
      "step = 8600, loss = 356.2996826171875\n",
      "step = 8700, loss = 355.1708984375\n",
      "step = 8800, loss = 354.02685546875\n",
      "step = 8900, loss = 352.93743896484375\n",
      "step = 9000, loss = 351.933837890625\n",
      "step = 9100, loss = 350.9857482910156\n",
      "step = 9200, loss = 350.0750427246094\n",
      "step = 9300, loss = 349.1589050292969\n",
      "step = 9400, loss = 348.2283630371094\n",
      "step = 9500, loss = 347.2904052734375\n",
      "step = 9600, loss = 346.40057373046875\n",
      "step = 9700, loss = 345.4122009277344\n",
      "step = 9800, loss = 344.422119140625\n",
      "step = 9900, loss = 343.5929260253906\n",
      "train_accuracy=0.8840000033378601\n",
      "test_accuracy=0.6010000109672546\n"
     ]
    }
   ],
   "source": [
    "lossi = train(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb84ac9580>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the same MLP layer but with fully pytorch code (nn.Linear(), etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network dimensions\n",
    "n_in = 784\n",
    "n_hidden = 200\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr = train_input, train_target\n",
    "X_test, Y_test = test_input, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList((\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_out),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __parameters__(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters]\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.13261\taccuracy (train, test): 0.13300\t0.16400\n",
      "step =   1000\tloss=0.00007\taccuracy (train, test): 1.00000\t0.87300\n",
      "step =   2000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.87300\n",
      "step =   3000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.88000\n",
      "step =   4000\tloss=0.00002\taccuracy (train, test): 1.00000\t0.88500\n",
      "step =   5000\tloss=0.00008\taccuracy (train, test): 1.00000\t0.88600\n",
      "step =   6000\tloss=0.00006\taccuracy (train, test): 1.00000\t0.88500\n",
      "step =   7000\tloss=0.00002\taccuracy (train, test): 1.00000\t0.88700\n",
      "step =   8000\tloss=0.00004\taccuracy (train, test): 1.00000\t0.88600\n",
      "step =   9000\tloss=0.00005\taccuracy (train, test): 1.00000\t0.88500\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: try to improve accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to improve the accuracy, I used a deeper neural network. I first tried to add a penalty so that the network generalizes better (since I suspect my network of overfitting the data due to the gap between the training and the testing error); however, I got worse results. I then tried to consider a deeper neural network and modified the loss function. I ended up with slightly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_deeper = 300\n",
    "class MLP_deeper(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList((\n",
    "            nn.Linear(n_in, n_hidden_deeper),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden_deeper, n_out),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __parameters__(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters]\n",
    "\n",
    "model_depper = MLP_deeper()\n",
    "optimizer = torch.optim.AdamW(model_depper.parameters(), lr=1e-3)\n",
    "l1_lambda = 0.01\n",
    "loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.25122\taccuracy (train, test): 0.12000\t0.20300\n",
      "step =   1000\tloss=0.02826\taccuracy (train, test): 0.99900\t0.88600\n",
      "step =   2000\tloss=0.02038\taccuracy (train, test): 1.00000\t0.88300\n",
      "step =   3000\tloss=0.01378\taccuracy (train, test): 1.00000\t0.88700\n",
      "step =   4000\tloss=0.01330\taccuracy (train, test): 1.00000\t0.88400\n",
      "step =   5000\tloss=0.01183\taccuracy (train, test): 1.00000\t0.88200\n",
      "step =   6000\tloss=0.01106\taccuracy (train, test): 1.00000\t0.88100\n",
      "step =   7000\tloss=0.01197\taccuracy (train, test): 1.00000\t0.88200\n",
      "step =   8000\tloss=0.01104\taccuracy (train, test): 1.00000\t0.87800\n",
      "step =   9000\tloss=0.01113\taccuracy (train, test): 1.00000\t0.87700\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model_depper(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model_depper(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
