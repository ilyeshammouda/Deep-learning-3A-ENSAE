{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Self-attention by hand\n",
    "# 2. Self-attention block in pytorch\n",
    "# 3. GPT, piece-by-piece\n",
    "# 4. GPU goes rrrr!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Self-attention by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- Write the scaled dot product self attention\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Scale the dot products\n",
    "  # 4. Apply softmax to calculate attentions\n",
    "  # 5. Weight values by attentions\n",
    "  # 6. Compute attention weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose values for the parameters\n",
    "# Do not modify this code\n",
    "\n",
    "X = np.array([[2,0,0,2],[0,1,0,0],[0,2,1,0],[0,0,1,1],[2,0,0,0],[1,0,1,1]], dtype=float)\n",
    "W_Q = np.array([[1,1,0,0,0,0],[0,1,0,1,0,0],[0,0,1,0,1,1]], dtype=float)\n",
    "W_K = np.array([[0,0,1,0,0,0],[0,1,0,0,0,0],[1,0,0,0,0,-1]], dtype=float)\n",
    "W_V = np.array([[10,0,0,0,0,0],[0,0,0,10,0,0],[0,10,0,0,0,0]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the first dimension of matrices Q and K correspond to?\n",
    "# The Head size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is given for you. Do not modify this code.\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in)\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0)\n",
    "  # Replicate denominator to N rows\n",
    "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the weighted attention matrix S\n",
    "Q= W_Q@X\n",
    "K= W_K@X\n",
    "S = softmax_cols(np.transpose(K)@Q/np.sqrt(Q.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the self-attention matrix A\n",
    "V= W_V@X\n",
    "A = V@S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "np.allclose(A, np.array([[10.30759701, 10.10551833, 15.03361159,  3.06082018],\n",
    "       [ 2.83283874,  2.97334971,  4.13169018,  1.53041009],\n",
    "       [ 4.59026201,  4.50027071,  2.10990693,  7.70438486]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Self-attention block in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this code\n",
    "\n",
    "batch_size = 3 # B\n",
    "block_size = 2 # T\n",
    "n_embd = 3     # C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a scaled self-attention head without masked attention and without dropout (i.e. just key, query and values)\n",
    "# A matrix multiplication is implemented using the nn.Linear() operator with no bias.\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.K = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.Q = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.V=nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "    def forward (self, x):\n",
    "        B, T, C = x.shape\n",
    "        q=self.Q(x)\n",
    "        k=self.K(x)\n",
    "        v=self.V(x)\n",
    "        print(q.shape, k.shape, v.shape)\n",
    "        out = F.softmax(q@k.transpose(-2,-1)/np.sqrt(C), dim=-1)@v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2]) torch.Size([3, 2, 2]) torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.46653441,  0.03306433],\n",
       "         [-0.47224301,  0.04610372]],\n",
       "\n",
       "        [[-0.38105938,  0.02397405],\n",
       "         [-0.39453450,  0.02482041]],\n",
       "\n",
       "        [[-0.29578221,  0.12158968],\n",
       "         [-0.30042297,  0.12526260]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "h = Head(2)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size, block_size, n_embd))\n",
    "out = h(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.46653444,  0.03306433],\n",
    "         [-0.47224304,  0.04610372]],\n",
    "        [[-0.38105938,  0.02397406],\n",
    "         [-0.39453450,  0.02482042]],\n",
    "        [[-0.29578221,  0.12158968],\n",
    "         [-0.30042297,  0.12526260]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weighted masked attention and dropout. Dropout comes after the softmax and before the multiplication with the value matrix.\n",
    "# Copy the Head class from the previous exercise and expand upon it.\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.K = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.Q = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.V=nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # store a persistent buffer for the forward pass\n",
    "\n",
    "    def forward (self, x):\n",
    "        B, T, C = x.shape\n",
    "        q=self.Q(x)\n",
    "        k=self.K(x)\n",
    "        v=self.V(x)\n",
    "        wei=q@k.transpose(-2,-1)/np.sqrt(C)\n",
    "        wei=wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        out = wei@v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.37939817, -0.16596894],\n",
       "         [-0.47224301,  0.04610372]],\n",
       "\n",
       "        [[-0.14184165,  0.00894911],\n",
       "         [-0.39453450,  0.02482041]],\n",
       "\n",
       "        [[-0.17301908,  0.02442870],\n",
       "         [-0.30042297,  0.12526260]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "h = Head(2)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size, block_size, n_embd))\n",
    "out = h(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.37939820, -0.16596894],\n",
    "         [-0.47224304,  0.04610372]],\n",
    "        [[-0.14184165,  0.00894911],\n",
    "         [-0.39453450,  0.02482042]],\n",
    "        [[-0.17301908,  0.02442869],\n",
    "         [-0.30042297,  0.12526260]]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A multi-head attention module contains a list of heads and a linear projection layer.\n",
    "# The heads are applied to the input and then concatenated along the last dimension, then\n",
    "# the linear layer is applied. Look at the unit test below to determine the dimensions of\n",
    "# the linear layer.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(num_heads*head_size, n_embd)\n",
    "        \n",
    "\n",
    "    def forward (self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify\n",
    "num_heads = 3\n",
    "head_size = 2\n",
    "n_embd = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "sa = MultiHeadAttention(num_heads=3, head_size=head_size)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size, block_size, n_embd))\n",
    "out = sa(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.03730504, -0.07006130, -0.27096999,  0.13144857, -0.45049590,\n",
    "          -0.33217290],\n",
    "         [-0.06794342, -0.04509801, -0.34738648,  0.15599491, -0.45456851,\n",
    "          -0.33087400]],\n",
    "        [[-0.08914752, -0.03846309, -0.36569631,  0.09802882, -0.39963537,\n",
    "          -0.29225215],\n",
    "         [-0.04709741,  0.01406255, -0.25430590,  0.08396727, -0.41786054,\n",
    "          -0.30781299]],\n",
    "        [[ 0.15234883, -0.08591781, -0.10099770,  0.19886394, -0.49236685,\n",
    "          -0.43605998],\n",
    "         [ 0.15425430, -0.01792544, -0.00511202,  0.14046597, -0.48078871,\n",
    "          -0.40730378]]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a classical feedforward module: linear -> ReLU -> linear\n",
    "# The hidden dimension is four times bigger than the input dimension (see Section 3.3 of Attention is All You Need)\n",
    "#\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__() \n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.58034134,  0.04641047, -0.10707696,  0.21581653, -0.30361828,\n",
       "         -0.07352638],\n",
       "        [-0.48917407,  0.07879594, -0.15972012,  0.17862342, -0.37070659,\n",
       "         -0.07852858],\n",
       "        [-0.48530388,  0.09604470, -0.06524839,  0.16611034, -0.35499069,\n",
       "         -0.08964306]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "ff = FeedForward(n_embd)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((3,n_embd))\n",
    "out = ff(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[-0.58034140,  0.04641046, -0.10707694,  0.21581653, -0.30361831,\n",
    "         -0.07352637],\n",
    "        [-0.48917407,  0.07879593, -0.15972012,  0.17862344, -0.37070659,\n",
    "         -0.07852858],\n",
    "        [-0.48530388,  0.09604470, -0.06524836,  0.16611034, -0.35499069,\n",
    "         -0.08964306]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a self-attention block\n",
    "#\n",
    "#   in -----> LayerNorm -------> multi-head attention -- + ----> LayerNorm -----> FeedForward --- + -----> out\n",
    "#         |                                              |   |                                    |\n",
    "#          ----------------------------------------------     ------------------------------------                       \n",
    "#\n",
    "# This architecture is slightly different from Attention is All You Need (or the UDL textbook):\n",
    "# the layer norm comes before (not after) the attention or feed-forward\n",
    "#\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.mH=MultiHeadAttention(n_head,head_size)\n",
    "        self.layer1=nn.LayerNorm(n_embd)\n",
    "        self.ff=FeedForward(n_embd)\n",
    "        self.layer2=nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.mH(self.layer1(x)) + x\n",
    "        out=self.ff(self.layer2(y)) + y\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.05278995, -0.10863629, -0.09458941,  0.97590691, -0.55101192,\n",
       "           0.57085061],\n",
       "         [-0.17928872, -0.44799614, -0.26547033,  1.11293721, -0.34837404,\n",
       "           0.40728986]],\n",
       "\n",
       "        [[-0.41515028, -0.30126402, -0.11399305,  0.64651299, -0.51579154,\n",
       "           0.57017863],\n",
       "         [-0.02734706,  0.08873296,  0.65776676,  0.70304358,  0.05667022,\n",
       "           0.70008963]],\n",
       "\n",
       "        [[ 0.52881181,  0.34458160,  0.31130394,  1.11564195,  0.37998515,\n",
       "          -0.02971911],\n",
       "         [ 1.39032197,  0.58906519,  0.97761846,  0.38604790,  0.63349819,\n",
       "           0.50254494]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test. Do not modify this code\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "bk = Block(n_embd, num_heads)\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "x = torch.rand((batch_size,block_size,n_embd))\n",
    "out = bk(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[[-0.05278997, -0.10863629, -0.09458938,  0.97590691, -0.55101192,\n",
    "           0.57085067],\n",
    "         [-0.17928867, -0.44799608, -0.26547045,  1.11293721, -0.34837404,\n",
    "           0.40728986]],\n",
    "        [[-0.41515028, -0.30126408, -0.11399293,  0.64651299, -0.51579159,\n",
    "           0.57017863],\n",
    "         [-0.02734703,  0.08873296,  0.65776664,  0.70304352,  0.05667025,\n",
    "           0.70008957]],\n",
    "        [[ 0.52881187,  0.34458166,  0.31130391,  1.11564195,  0.37998506,\n",
    "          -0.02971917],\n",
    "         [ 1.39032197,  0.58906519,  0.97761846,  0.38604784,  0.63349819,\n",
    "           0.50254500]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Build a mini GPT\n",
    "#\n",
    "# - Start from the gpt-problem.py file\n",
    "# - Add your Head, MultiHeadAttention, FeedForward and Block classes\n",
    "# - Fill in the GPT class (__init__ and forward methods)\n",
    "# - Train the network on CPU\n",
    "# - Train the network on GPU\n",
    "\n",
    "# For __init__, the GPT model parameters are:\n",
    "#   - a token embedding table\n",
    "#   - a positional embedding table\n",
    "#   - a sequence of Blocks\n",
    "#   - a layer norm\n",
    "#   - a linear layer\n",
    "#\n",
    "# For forward(), the model consists in:\n",
    "#   - applying the token embedding table and positional embedding table to the input tensor\n",
    "#   - adding the two together\n",
    "#   - applying the blocks, layer norm and linear layer (in that order)\n",
    "#\n",
    "# The code comes from hyperparameters that should work well on GPU.  On CPU, you \n",
    "# will need to reduce the model size significantly.\n",
    "#\n",
    "# In pytorch, an learnable embedding table is implemented with nn.Embedding(...)\n",
    "#\n",
    "# The token embedding table learns an embedding for each item of the vocabulary. The \n",
    "# positional embedding table does not depend on the input and learns an embedding\n",
    "# for each position in the context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
